# -*- coding:utf-8 _*-
import os
import shutil
import gradio as gr
from langchain_application import LangChainApplication


# ä¿®æ”¹æˆè‡ªå·±çš„é…ç½®ï¼ï¼ï¼
class LangChainCFG:
    llm_model_name = '..\\chatglm-6b-noint'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“ THUDM/chatglm-6b
    embedding_model_name = '..\\text2vec-large-chinese'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“ GanymedeNil/text2vec-large-chines
    vector_store_path = './cache'
    docs_path = './docs'
    patterns = ['æ¨¡å‹é—®ç­”', 'çŸ¥è¯†åº“é—®ç­”']
    n_gpus=1

config = LangChainCFG()
application = LangChainApplication(config)


if not os.path.exists(config.docs_path):
    os.mkdir(config.docs_path)

file_list = [f for f in os.listdir("docs")]

application.source_service.init_source_vector(file_list)

def upload_file(file):
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    file_list.insert(0, filename)
    application.source_service.add_document("docs/" + filename)

def clear_session():
    return '', None


def predict(input,
            large_language_model,
            embedding_model,
            top_k,
            use_pattern,
            history=None):
    if history == None:
        history = []


    if use_pattern == 'æ¨¡å‹é—®ç­”':
        result = application.get_llm_answer(query=input)
        history.append((input, result))
        return '', history, history, ''

    else:
        search_text = 'æ£€æŸ¥ç›¸å…³çš„æ–‡ä»¶å¦‚ä¸‹ï¼š\n'
        resp = application.get_knowledge_based_answer(
            query=input,
            history_len=1,
            temperature=0.1,
            top_p=0.9,
            top_k=top_k,
            chat_history=history
        )
        history.append((input, resp['result']))
        print(enumerate(resp['source_documents']))
        for idx, source in enumerate(resp['source_documents'][:top_k]):
            filename = source.metadata["filename"]
            search_text += f'{idx}. {filename}\n'
        return '', history, history, search_text

with gr.Blocks() as demo:
    gr.Markdown("""<h1><center>LangChain Engine</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):
            embedding_model = gr.Dropdown([
                "text2vec-base"
            ],
                label="Embedding model",
                value="text2vec-base")

            large_language_model = gr.Dropdown(
                [
                    "ChatGLM-6B-int4",
                ],
                label="large language model",
                value="ChatGLM-6B-int4")

            top_k = gr.Slider(1,
                              20,
                              value=2,
                              step=1,
                              label="æ£€ç´¢top-kæ–‡æ¡£",
                              interactive=True)

            use_pattern = gr.Radio(
                [
                    'æ¨¡å‹é—®ç­”',
                    'çŸ¥è¯†åº“é—®ç­”',
                ],
                label="æ¨¡å¼",
                value='æ¨¡å‹é—®ç­”',
                interactive=True)


            file = gr.File(label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…\r\n" \
                            "æ”¯æŒç±»å‹ä¸º txt, md, docx, pdf",
                           visible=True,
                           file_types=['.txt', '.md', '.docx', '.pdf']
                           )

        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(label='Chinese-LangChain').style(height=400)
            with gr.Row():
                message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
            with gr.Row():
                clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                send = gr.Button("ğŸš€ å‘é€")
        with gr.Column(scale=2):
            search = gr.Textbox(label='çŸ¥è¯†åº“æœç´¢ç»“æœ')

        # ============= è§¦å‘åŠ¨ä½œ=============
        file.upload(upload_file,
                    inputs=file,
                    outputs=None)
        # å‘é€æŒ‰é’® æäº¤
        send.click(predict,
                   inputs=[
                       message,
                       large_language_model,
                       embedding_model,
                       top_k,
                       use_pattern,
                       state
                   ],
                   outputs=[message, chatbot, state, search])

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(fn=clear_session,
                            inputs=[],
                            outputs=[chatbot, state],
                            queue=False)

        # è¾“å…¥æ¡† å›è½¦
        message.submit(predict,
                       inputs=[
                           message,
                           large_language_model,
                           embedding_model,
                           top_k,
                           use_pattern,
                           state
                       ],
                       outputs=[message, chatbot, state, search])

demo.queue(concurrency_count=2).launch(
    server_name='0.0.0.0',
    #server_port=8888,
    share=False,
    show_error=True,
    debug=True,
    enable_queue=True,
    inbrowser=True,
)
